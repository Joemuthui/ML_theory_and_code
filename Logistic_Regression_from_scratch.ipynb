{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression From Scratch Using Iris Data Set\n",
    "Logistic Regression is a classification algorithm that aims at predicting the probability that a certain observation $X_i$ belongs to the class $y=1$.\n",
    "\n",
    "Here we will first give the formulas that we will implement LR with using Gradient descent algorithm (Following the Likelihood approach).\n",
    "\n",
    "Generally,we want to find a function that gives the probability of a linear combination of the input variable/features $X$ i.e. $$p=\\theta_0+\\theta_1x_1+ .... +\\theta_nx_n$$. We know that a probability a probaility $0\\leq p\\leq 1$, from the function $p$ it is evident that the probility might be great than $1$ or even less than $0$.\n",
    "\n",
    "We can therefore, instead of assignig $p$ to the function we assign the odds i.e $\\frac{p}{1-p}$. Such that ,\n",
    " $$\\frac{p}{1-p}=\\theta_0+\\theta_1x_1+ .... +\\theta_nx_n$$\n",
    " \n",
    " We also know that $\\theta_0+\\theta_1x_1+ .... +\\theta_nx_n$ might sometimes be negative, to ensure that then negatives are taken care of we apply $\\log$ on $\\frac{p}{1-p}$.This leads to,\n",
    " $$\\log\\bigg(\\frac{p}{1-p}\\bigg)=\n",
    " \\theta_0+\\theta_1x_1+ .... +\\theta_nx_n=\\theta^TX \\quad \\text{(In vector notation)}$$\n",
    " \n",
    " Equation above ensures that the output is within the range of $0$ and $1$. To simplify things further and solve for $p$, we can exponentiate both sides of the equation above to obtain,\n",
    " \\begin{align}\n",
    " &\\frac{p}{1-p}=e^{\\theta^TX}\\\\\n",
    " &p=e^{\\theta^TX}-pe^{\\theta^TX}\\\\\n",
    " &p+pe^{\\theta^TX}=e^{\\theta^TX}\\\\\n",
    " &p\\bigg(1+e^{\\theta^TX}\\bigg)=e^{\\theta^TX}\\\\\n",
    " &p=\\frac{e^{\\theta^TX}}{1+e^{\\theta^TX}}\\\\\n",
    " &p=\\frac{1}{1+e^{-\\theta^TX}}\n",
    " \\end{align}\n",
    "The last equation is the famous sigmoid/logistic function.\n",
    "\n",
    "Since we have already found the probility $p$, which gives the probability that a certain observation $X_i$ belongs to a certain class, in this case $1$, then the probaility that that the obsercation $X_i$ belongs to the class $0$ is $1-p$. This is because we have only two classes $\\{1,0\\}$. This is represented as:\n",
    "$$P(y=1|X;\\theta)=p\\\\\n",
    "P(y=0|X;\\theta)=1-p$$\n",
    "For such an obsertion $X_i=x$, the probability above can be written in a more compact form as,$$P(y|x)=p^{y}(1-p)^{1-y}$$\n",
    "\n",
    "And since we have many observations in $X$, the to obtain the probality for all the observations, we multiply the individual probability with each other i.e.\n",
    "$$P(y|x)=p^{y_1}(1-p)^{1-y_1}\\times p^{y_2}(1-p)^{1-y_2}\\times...\\times p^{y_n}(1-p)^{1-y_n\n",
    "}$$\n",
    "For $n$ observations. The product above is called the Likelihood (which needs to be minimized/optimized)of a certain outcome given inputs. It written as,\n",
    "$$L(y;x)=\\prod_{i=1}^{n}p^{y_i}(1-p)^{1-y_i}$$\n",
    "\n",
    "But we know that GradienT Descent (GD) uses derivatives, therefore it is wise not find derivatives of products (Not easy). We therefore apply logarithm on the likelihood to obtain the log-likelihood (l)\n",
    "$$l=\\log(L(y;x))=\\log\\bigg(\\prod_{i=1}^{n}p^{y_i}(1-p)^{1-y_i}\\bigg)=\\frac{1}{n}\\sum_{i=1}^{n}y_i\\log(p)+(1-y_i)\\log(1-p)$$\n",
    "\n",
    "Recall that $$p=\\frac{1}{1+e^{-\\theta^TX}}$$\n",
    "We can therefore find the partial derivatives of $\\theta_j$, since we want to to find the parameters $\\theta_j$ that optimizes the log-likelihood function. \n",
    "\n",
    "After applying chain rule on the log-likelihood function,we obtain\n",
    "$$\\frac{\\partial l}{\\partial \\theta_j}=\\frac{1}{n}\\sum_{i=1}^{n}X_i(p-y_i)$$\n",
    "\n",
    "To find the optimum $\\theta_j$ we use GD, which iteratively updates the values of \\theta_j untill convergence using:\n",
    "$$\\theta_j \\text(new)=\\theta_j \\text(old)-\\alpha \\frac{\\partial l}{\\partial \\theta_j}$$\n",
    "where alpha is learning rate choosen experimentaly.\n",
    "\n",
    "Once the optimum values of $ \\theta_j$ is found can use $p=\\frac{1}{1+e^{-\\theta^TX}}$ to classify unseen data $X$.\n",
    "\n",
    "Let us now implement Binary Logistic Regression from scratch and also using inbuilt package. We will use data from kaggle Iris data but with two categories instead of three. Kindly download the data here. If you want to use the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary packages\n",
    "import numpy as np # performing array calculations\n",
    "import pandas as pd # Load and manipulating the dataset\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt # visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOad the data which is local in my machine\n",
    "data=pd.read_csv('Iris.csv')\n",
    "#check the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the shape of our dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data has four features $X=\\{SepalLengthC,SepalWidthCm,PetalLengthCm PetalWidthCm\\}$  and one Target variable $y=Species$,   with $n=150$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The species has how namy unique categories\n",
    "data['Species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As seen above,There are three categories ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'). We remove one so that its binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor']\n",
      "(100, 6)\n"
     ]
    }
   ],
   "source": [
    "#we remove one species using pandas\n",
    "data1=data[data['Species']!='Iris-virginica']\n",
    "print(data1.Species.unique() )#Two classes are left.\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now $n=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   SepalLengthCm  100 non-null    float64\n",
      " 1   SepalWidthCm   100 non-null    float64\n",
      " 2   PetalLengthCm  100 non-null    float64\n",
      " 3   PetalWidthCm   100 non-null    float64\n",
      " 4   Species        100 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 4.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#Next we drop the id column and check whether there missing values in the dataset\n",
    "data2=data1.drop('Id', axis=1)\n",
    "data2.info() \n",
    "#No null /or missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step we convert the species column to 0 and 1  since it is of onject datatype using the Label Encoder from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode the Species/Target Vaiable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data2['Species']=le.fit_transform(data2['Species'])\n",
    "data2.Species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we see that Iris-setosa was encoded as zero and Iris-versicolor as 1.\n",
    "\n",
    "#### The data appears to be recorded in a certain order we shuffle to mix the data well.  Uisng the sample method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data2.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now assing X and y their respective values.\n",
    "X=data2.drop('Species',axis=1).values\n",
    "y=data2.Species.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the dataset isn't that large we shall only train and check the score on the training data. Remember that it is of paramount to have a testing set in order to have a better model. In this tutorial, we just want to show how Logistic regression works with GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hstack a column onf ones in X\n",
    "ones=np.ones([100,1])\n",
    "X=np.hstack([ones,X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the weights\n",
    "weights=np.zeros(5)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000\n",
    "cost=[]\n",
    "def sigmoid(p):\n",
    "    return 1/(1+np.exp(-p))\n",
    "def gradient(X,y,p):\n",
    "    return 1/X.shape[0]*(np.dot(X.T,p-y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X,y,epochs,alpha):\n",
    "    weights=np.zeros(5)\n",
    "    for i in range(epochs):\n",
    "        product=np.dot(X,weights)\n",
    "        #print(product)\n",
    "        p=sigmoid(product)\n",
    "        l=-1/X.shape[0]*np.sum(y*np.log(p)+(1-y)*np.log(1-p))\n",
    "        #print(l)\n",
    "        cost.append(l)\n",
    "        weights=weights-alpha*gradient(X,y,p)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1=logistic(X,y,500,0.02)\n",
    "diff=np.round(sigmoid(X@w1))-y \n",
    "(len(y)-np.count_nonzero(diff))/len(y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.pinv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
